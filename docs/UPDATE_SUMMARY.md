# æ¡†æ¶æ›´æ–°æ€»ç»“

æ ¹æ®ä½ çš„è¦æ±‚ï¼Œæˆ‘å·²ç»å®Œæˆäº†ä»¥ä¸‹4é¡¹ä¿®æ­£å’Œè¡¥å……ï¼š

## âœ… 1. è¡¥è¶³ ExperimentRunner çš„ run_single å‡½æ•°

### æ›´æ–°å†…å®¹

- **åˆ›å»ºäº† `complete_experiment_runner.py`** - å®Œæ•´çš„å®éªŒè¿è¡Œå™¨
- **å®ç°äº†å®Œæ•´çš„å®éªŒæµç¨‹**ï¼š
  1. Baselineè¿è¡Œï¼ˆæ— é”™è¯¯æ³¨å…¥ï¼‰
  2. Injectionè¿è¡Œï¼ˆå¸¦é”™è¯¯æ³¨å…¥ï¼‰
  3. è¾¹ç•Œè®¡ç®—
  4. è¿åæ£€æµ‹
  5. æ€§èƒ½ç›‘æ§

### æ ¸å¿ƒç»„ä»¶

```python
class CompleteExperimentRunner:
    def run_single(self, run_id, model, dataloader):
        # 1. è¿è¡Œbaseline
        baseline_outputs, baseline_tensors = run_model_with_collection(...)
        
        # 2. è¿è¡Œinjection
        injected_outputs, injected_tensors = run_model_with_collection(...)
        
        # 3. è®¡ç®—è¾¹ç•Œ
        bounds = compute_attention_bounds(scores, weights, d)
        
        # 4. æ£€æµ‹è¿å
        detection = detect_violation(bounds, injected_eps, tolerance)
        
        return result
```

### å…³é”®ç‰¹æ€§

- âœ… è‡ªåŠ¨æ”¶é›†ä¸­é—´å¼ é‡ (q, k, v, scores, weights)
- âœ… æ”¯æŒå¤šå±‚å¤„ç†
- âœ… å®Œæ•´çš„æ€§èƒ½ç›‘æ§
- âœ… ç»“æœè‡ªåŠ¨ä¿å­˜

---

## âœ… 2. æ”¯æŒå¤šå±‚æ³¨å…¥é…ç½®

### æ›´æ–°å†…å®¹

#### 2.1 é…ç½®å±‚é¢ (`experiment_config.py`)

æ·»åŠ äº†æ–°çš„é…ç½®å‚æ•°ï¼š

```python
@dataclass
class ExperimentConfig:
    # å±‚é€‰æ‹©é…ç½®
    injection_layers: Optional[List[int]] = None  # è¦æ³¨å…¥çš„å±‚ç´¢å¼•
    injection_layer_mode: str = "first"  # first, all, random, specific
```

#### 2.2 æ¨¡å‹é€‚é…å±‚é¢ (`model_adapter.py`)

æ›´æ–°äº† `monkey_patch_model` å‡½æ•°ï¼š

```python
def monkey_patch_model(model, model_type: str, 
                       injection_layers: Optional[List[int]] = None,
                       force_kv_equal: bool = False):
    """
    Args:
        injection_layers: è¦æ³¨å…¥çš„å±‚ç´¢å¼•åˆ—è¡¨
            - None: æ‰€æœ‰å±‚
            - [0, 1, 2]: åªåœ¨è¿™äº›å±‚æ³¨å…¥
    """
```

### ä½¿ç”¨ç¤ºä¾‹

```python
# åªåœ¨ç¬¬0å±‚æ³¨å…¥
config.injection_layers = [0]

# åœ¨å‰3å±‚æ³¨å…¥
config.injection_layers = [0, 1, 2]

# åœ¨ç‰¹å®šå±‚æ³¨å…¥
config.injection_layers = [0, 3, 6, 9]

# æ‰€æœ‰å±‚æ³¨å…¥
config.injection_layers = None
```

---

## âœ… 3. æ·»åŠ æ€§èƒ½æ£€æµ‹æ¨¡å—

### æ–°å¢æ–‡ä»¶ï¼š`performance_monitor.py`

#### 3.1 æ€§èƒ½æŒ‡æ ‡ç±»

```python
@dataclass
class PerformanceMetrics:
    # æ—¶é—´æŒ‡æ ‡
    total_time: float
    baseline_forward_time: float
    injection_forward_time: float
    bounds_computation_time: float
    violation_detection_time: float
    
    # å†…å­˜æŒ‡æ ‡
    peak_memory_allocated: float  # MB
    peak_memory_reserved: float
    baseline_memory: float
    injection_memory: float
    
    # è®¡ç®—æŒ‡æ ‡
    num_attention_layers: int
    num_attention_heads: int
    sequence_length: int
    batch_size: int
    attention_flops: float
    bounds_flops: float
```

#### 3.2 æ€§èƒ½ç›‘æ§å™¨

```python
class PerformanceMonitor:
    # è®¡æ—¶å™¨
    with monitor.timer('baseline_forward'):
        ...
    
    # å†…å­˜è®°å½•
    monitor.record_memory('baseline')
    
    # æ¨¡å‹ä¿¡æ¯
    monitor.record_model_info(model, batch_size, seq_length)
    
    # FLOPSä¼°ç®—
    monitor.estimate_flops(d_model, n_heads, seq_len, batch_size)
```

#### 3.3 æ€§èƒ½èšåˆå™¨

ç”¨äºå¤šæ¬¡å®éªŒçš„ç»Ÿè®¡åˆ†æï¼š

```python
class PerformanceAggregator:
    def add(self, metrics)
    def compute_statistics()  # mean, std, min, max, median
    def print_statistics()
```

### ç›‘æ§çš„æŒ‡æ ‡

| ç±»åˆ« | æŒ‡æ ‡ | è¯´æ˜ |
|------|------|------|
| **æ—¶é—´** | baseline_forward_time | Baselineå‰å‘ä¼ æ’­æ—¶é—´ |
| | injection_forward_time | æ³¨å…¥ç‰ˆå‰å‘ä¼ æ’­æ—¶é—´ |
| | bounds_computation_time | è¾¹ç•Œè®¡ç®—æ—¶é—´ |
| | violation_detection_time | è¿åæ£€æµ‹æ—¶é—´ |
| | total_time | æ€»æ—¶é—´ |
| **å†…å­˜** | peak_memory_allocated | å³°å€¼å·²åˆ†é…å†…å­˜ (MB) |
| | peak_memory_reserved | å³°å€¼é¢„ç•™å†…å­˜ (MB) |
| | baseline_memory | Baselineå†…å­˜ä½¿ç”¨ |
| | injection_memory | æ³¨å…¥ç‰ˆå†…å­˜ä½¿ç”¨ |
| **å¼€é”€** | injection_vs_baseline | æ³¨å…¥vsåŸºçº¿çš„æ—¶é—´å¼€é”€ (%) |
| | detection_vs_baseline | æ£€æµ‹vsåŸºçº¿çš„æ—¶é—´å¼€é”€ (%) |
| | total_vs_baseline | æ€»å¼€é”€ (%) |

### è¾“å‡ºç¤ºä¾‹

```
==============================================================
Performance Summary
==============================================================

[Time Metrics]
  Total time:              2.3456s
  Baseline forward:        0.8234s
  Injection forward:       0.8567s
  Bounds computation:      0.4123s
  Violation detection:     0.1234s

[Overhead]
  injection_vs_baseline: +4.04%
  detection_vs_baseline: +65.09%
  total_vs_baseline: +69.13%

[Memory Metrics]
  Peak allocated:          512.34 MB
  Peak reserved:           1024.00 MB
  Baseline memory:         487.23 MB
  Injection memory:        489.12 MB

[Model Info]
  Attention layers:        12
  Attention heads:         12
  Sequence length:         128
  Batch size:              4

[FLOPS Estimate]
  Attention:               2.51e+09
  Bounds computation:      6.29e+06
  Effective TFLOPS:        3.05
==============================================================
```

---

## âœ… 4. KVæƒé‡ä¸€è‡´æ€§å¤„ç†

### 4.1 æ£€æŸ¥KVä¸€è‡´æ€§

æ–°å¢å‡½æ•° `check_kv_consistency()`:

```python
def check_kv_consistency(model, model_type: str) -> Dict:
    """
    æ£€æŸ¥æ¨¡å‹çš„Kå’ŒVæƒé‡æ˜¯å¦ä¸€è‡´
    
    Returns:
        {
            'kv_equal': bool,  # æ˜¯å¦æ‰€æœ‰å±‚K=V
            'layer_info': [
                {
                    'layer_idx': int,
                    'kv_equal': bool,
                    'k_shape': list,
                    'v_shape': list
                },
                ...
            ]
        }
    """
```

### 4.2 ä½¿ç”¨ç¤ºä¾‹

```python
# 1. åŠ è½½é¢„è®­ç»ƒGPT-2
model = GPT2LMHeadModel.from_pretrained('gpt2')

# 2. æ£€æŸ¥KVä¸€è‡´æ€§
kv_check = check_kv_consistency(model, 'gpt2')

if kv_check['kv_equal']:
    print("âœ“ K and V are equal - can use tight bounds")
else:
    print("âš ï¸  K and V are NOT equal")
    print("Options:")
    print("1. Use relaxed bounds (current)")
    print("2. Force K=V by weight sharing (experimental)")
    print("3. Train new model with K=V constraint")
```

### 4.3 å…³äºé¢„è®­ç»ƒGPT-2çš„KVæƒé‡

**å®é™…æƒ…å†µï¼š**
- âŒ GPT-2çš„é¢„è®­ç»ƒæ¨¡å‹ **K â‰  V**
- âœ… GPT-2ä½¿ç”¨ç‹¬ç«‹çš„QKVçº¿æ€§å±‚
- âš ï¸  ç†è®ºè¾¹ç•Œå¯èƒ½ä¸å¤Ÿç´§è‡´

**è§£å†³æ–¹æ¡ˆï¼š**

#### é€‰é¡¹1ï¼šä½¿ç”¨å½“å‰æ¡†æ¶ï¼ˆæ¨èï¼‰âœ…

```python
# ç›´æ¥ä½¿ç”¨ï¼Œä¸å¼ºåˆ¶K=V
model = monkey_patch_model(
    model, 
    'gpt2', 
    force_kv_equal=False  # ä½¿ç”¨æ”¾æ¾çš„è¾¹ç•Œ
)
```

**ä¼˜ç‚¹ï¼š**
- ç«‹å³å¯ç”¨ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ
- é€‚åˆå¿«é€ŸåŸå‹å’Œå®éªŒ
- ä»ç„¶å¯ä»¥æ£€æµ‹åˆ°å¤§éƒ¨åˆ†é”™è¯¯

**ç¼ºç‚¹ï¼š**
- è¾¹ç•Œå¯èƒ½è¾ƒæ¾
- æ£€æµ‹çµæ•åº¦å¯èƒ½é™ä½

#### é€‰é¡¹2ï¼šå¼ºåˆ¶K=Vï¼ˆå®éªŒæ€§ï¼‰âš ï¸

```python
model = monkey_patch_model(
    model, 
    'gpt2', 
    force_kv_equal=True  # å¼ºåˆ¶å…±äº«æƒé‡
)
```

**æ³¨æ„ï¼š** æ­¤åŠŸèƒ½å°šæœªå®Œå…¨å®ç°ï¼Œéœ€è¦ï¼š
- ä¿®æ”¹æ¨¡å‹æƒé‡ä½¿Kå’ŒVå…±äº«
- å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½
- éœ€è¦é¢å¤–çš„æµ‹è¯•

#### é€‰é¡¹3ï¼šé‡æ–°è®­ç»ƒï¼ˆæœ€ä¼˜ä½†è€—æ—¶ï¼‰ğŸ¯

è®­ç»ƒä¸€ä¸ªK=Vçš„æ¨¡å‹ï¼š

```python
# è‡ªå®šä¹‰GPT-2é…ç½®
config = GPT2Config(...)

# ä¿®æ”¹attentionå±‚ä½¿K=V
class CustomGPT2Attention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_q = nn.Linear(...)
        self.c_kv = nn.Linear(...)  # Kå’ŒVå…±äº«
```

---

## ğŸ“ æ–°å¢/ä¿®æ”¹çš„æ–‡ä»¶

### æ–°å¢æ–‡ä»¶

1. âœ¨ **performance_monitor.py** - æ€§èƒ½æ£€æµ‹æ¨¡å—
2. âœ¨ **complete_experiment_runner.py** - å®Œæ•´å®éªŒè¿è¡Œå™¨
3. âœ¨ **full_example.py** - å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
4. ğŸ“ **UPDATE_SUMMARY.md** - æœ¬æ–‡æ¡£

### ä¿®æ”¹æ–‡ä»¶

1. ğŸ”§ **experiment_config.py** - æ·»åŠ å±‚é€‰æ‹©é…ç½®
2. ğŸ”§ **model_adapter.py** - æ·»åŠ å¤šå±‚æ”¯æŒå’ŒKVæ£€æŸ¥
3. ğŸ”§ **experiment_runner.py** - æ›´æ–°run_singleæ¡†æ¶

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„ä¾‹å­

```python
# 1. å®‰è£…ä¾èµ–
pip install torch transformers datasets scipy numpy psutil

# 2. è¿è¡Œç®€å•å®éªŒ
python full_example.py simple

# 3. æŸ¥çœ‹ç»“æœ
ls results/gpt2_simple_test_*/
```

### å®Œæ•´å®éªŒæµç¨‹

```python
from experiment_config import ExperimentConfig
from model_adapter import monkey_patch_model, check_kv_consistency
from complete_experiment_runner import CompleteExperimentRunner
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# 1. é…ç½®
config = ExperimentConfig(
    exp_name="my_exp",
    injection_layers=[0, 1, 2],  # å‰3å±‚
    injection_location="scores",
    injection_bit=15
)

# 2. åŠ è½½æ¨¡å‹
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 3. æ£€æŸ¥KV
kv_check = check_kv_consistency(model, 'gpt2')
print(f"K=V? {kv_check['kv_equal']}")

# 4. Patchæ¨¡å‹
model = monkey_patch_model(
    model, 
    'gpt2',
    injection_layers=config.injection_layers
)

# 5. å‡†å¤‡æ•°æ®
# ... (è§full_example.py)

# 6. è¿è¡Œå®éªŒ
runner = CompleteExperimentRunner(config)
results = runner.run_all(model, dataloader)

# 7. æŸ¥çœ‹ç»“æœ
for result in results:
    print(f"Loss diff: {result.loss_diff}")
    print(f"Violations: {result.num_violations}")
```

---

## ğŸ“Š å®éªŒå»ºè®®

### å¯¹äºKVä¸ä¸€è‡´çš„é¢„è®­ç»ƒæ¨¡å‹

#### å®éªŒè®¾ç½®

```python
config = ExperimentConfig(
    # ä½¿ç”¨è¾ƒå¤§çš„tolerance
    tolerance=1e-5,  # è€Œä¸æ˜¯1e-6
    
    # å¤šæ¬¡è¿è¡Œä»¥è·å¾—ç»Ÿè®¡
    num_runs=10,
    
    # æ‰«æå¤šä¸ªæ¯”ç‰¹ä½
    # (æŸäº›æ¯”ç‰¹ä½å¯èƒ½æ›´å®¹æ˜“æ£€æµ‹)
)

sweep_params = {
    'injection_bit': list(range(32)),
    'injection_location': ['scores', 'weights', 'out'],
    'seed': [42, 123, 456, 789]
}
```

#### åˆ†æé‡ç‚¹

1. **æ£€æµ‹ç‡åˆ†æ**
   - å“ªäº›æ¯”ç‰¹ä½æ£€æµ‹ç‡é«˜ï¼Ÿ
   - å“ªäº›æ³¨å…¥ä½ç½®æ›´å®¹æ˜“æ£€æµ‹ï¼Ÿ

2. **è¾¹ç•Œç´§è‡´åº¦**
   - epsilonçš„åˆ†å¸ƒ
   - è¿åmarginçš„ç»Ÿè®¡

3. **æ€§èƒ½å¼€é”€**
   - æ£€æµ‹æ—¶é—´ vs åŸºçº¿æ—¶é—´
   - å†…å­˜å¼€é”€

---

## âš ï¸  å·²çŸ¥é™åˆ¶

1. **KVä¸ä¸€è‡´** - é¢„è®­ç»ƒGPT-2çš„Kâ‰ Vï¼Œè¾¹ç•Œå¯èƒ½è¾ƒæ¾
2. **ä¸­é—´å¼ é‡æ”¶é›†** - éœ€è¦é¢å¤–å†…å­˜ï¼Œå¤§æ¨¡å‹å¯èƒ½OOM
3. **å¤šå±‚æ³¨å…¥** - åŒæ—¶æ³¨å…¥å¤šå±‚æ—¶ï¼Œé”™è¯¯å¯èƒ½ä¼ æ’­å’Œå åŠ 
4. **å› æœæ©ç ** - å½“å‰å¯¹æ©ç çš„å¤„ç†å¯èƒ½ä¸å¤Ÿå®Œå–„

---

## ğŸ”œ åç»­å·¥ä½œ

### çŸ­æœŸï¼ˆ1å‘¨å†…ï¼‰

- [ ] å®Œå–„å› æœæ©ç çš„è¾¹ç•Œå¤„ç†
- [ ] ä¼˜åŒ–ä¸­é—´å¼ é‡æ”¶é›†çš„å†…å­˜ä½¿ç”¨
- [ ] æ·»åŠ æ›´å¤šæ¨¡å‹æ”¯æŒ (DistilBERT, OPT)

### ä¸­æœŸï¼ˆ2-4å‘¨ï¼‰

- [ ] å®ç°è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ³¨å…¥
- [ ] æ”¯æŒå¼ºåˆ¶K=Vçš„æƒé‡å…±äº«
- [ ] æ·»åŠ å¯è§†åŒ–å·¥å…·

### é•¿æœŸï¼ˆ1-3æœˆï¼‰

- [ ] è®­ç»ƒä¸€ä¸ªK=Vçš„æ¨¡å‹
- [ ] å¤§è§„æ¨¡å‚æ•°æ‰«æ
- [ ] ä¼˜åŒ–è¾¹ç•Œå…¬å¼ï¼ˆå¼•å…¥ä¿®æ­£å› å­ï¼‰

---

## ğŸ“š æ–‡æ¡£

- `README.md` - æ¡†æ¶æ€»ä½“æ–‡æ¡£
- `UPDATE_SUMMARY.md` - æœ¬æ–‡æ¡£
- `full_example.py` - å®Œæ•´ç¤ºä¾‹ä»£ç 
- `Logs.md` - ç†è®ºæ¨å¯¼

---

## ğŸ¯ å»ºè®®çš„å®éªŒé¡ºåº

```
1. è¿è¡Œ full_example.py simple
   â†“
2. æ£€æŸ¥ç»“æœï¼ŒéªŒè¯æ¡†æ¶æ­£å¸¸å·¥ä½œ
   â†“
3. è¿è¡Œå°è§„æ¨¡å‚æ•°æ‰«æ (5 bits Ã— 2 seeds)
   â†“
4. åˆ†æåˆæ­¥ç»“æœ
   â†“
5. è¿è¡Œä¸­ç­‰è§„æ¨¡æ‰«æ (32 bits Ã— 6 locations Ã— 4 seeds)
   â†“
6. æ ¹æ®ç»“æœè°ƒæ•´toleranceå’Œé…ç½®
   â†“
7. è¿è¡Œå¤§è§„æ¨¡å®éªŒ
```

---

ç°åœ¨ä½ æœ‰ä¸€ä¸ª**å®Œæ•´ã€å¯è¿è¡Œã€æ”¯æŒå¤šå±‚æ³¨å…¥å’Œæ€§èƒ½ç›‘æ§**çš„æ¡†æ¶äº†ï¼ğŸ‰