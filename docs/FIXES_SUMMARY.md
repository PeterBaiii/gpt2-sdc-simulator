# é—®é¢˜ä¿®å¤æ€»ç»“

## é—®é¢˜1ï¼šè¡¥å……æ•°æ®é›†åŠ è½½æ¨¡å— âœ…

### åˆ›å»ºçš„æ–‡ä»¶
**custom_datasets.py** - å®Œæ•´çš„æ•°æ®é›†åŠ è½½æ¨¡å—

### æä¾›çš„åŠŸèƒ½

#### 1. ä¸‰ç§æ•°æ®é›†å®ç°

| æ•°æ®é›†ç±»å‹ | ç±»å | ç”¨é€” |
|------------|------|------|
| HuggingFaceæ•°æ®é›† | `load_dataset_hf()` | ä½¿ç”¨å®˜æ–¹datasetsåº“ï¼ˆæ¨èï¼‰ |
| æœ¬åœ°WikiText | `WikiTextDataset` | ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ |
| ç®€å•æ–‡æœ¬ | `SimpleTextDataset` | è‡ªå®šä¹‰æ–‡æœ¬åˆ—è¡¨ |
| è™šæ‹Ÿæ•°æ® | `DummyDataset` | éšæœºç”Ÿæˆï¼Œç”¨äºå¿«é€Ÿæµ‹è¯• |

#### 2. ç»Ÿä¸€æ¥å£

```python
def load_dataset(
    dataset_name: str = 'wikitext',
    subset: Optional[str] = None,
    split: str = 'test',
    tokenizer = None,
    max_length: int = 128,
    max_samples: Optional[int] = None,
    use_hf: bool = True,
    local_path: Optional[str] = None
):
    """ç»Ÿä¸€çš„æ•°æ®é›†åŠ è½½æ¥å£"""
```

#### 3. ä¾¿æ·å‡½æ•°

```python
# æœ€å¸¸ç”¨ï¼šWikiText-2
dataloader = prepare_wikitext2(tokenizer, batch_size=4, max_samples=100)

# å¿«é€Ÿæµ‹è¯•ï¼šè™šæ‹Ÿæ•°æ®
dataloader = prepare_dummy_data(tokenizer, batch_size=4, num_samples=100)
```

### ä½¿ç”¨æ–¹å¼

#### æ–¹å¼1ï¼šä½¿ç”¨HuggingFace datasetsï¼ˆæ¨èï¼‰âœ¨

```python
from custom_datasets import load_dataset
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

# è‡ªåŠ¨ä¸‹è½½å’ŒåŠ è½½
dataset = load_dataset(
    dataset_name='wikitext',
    subset='wikitext-2-raw-v1',
    split='test',
    tokenizer=tokenizer,
    max_samples=100,
    use_hf=True  # ä½¿ç”¨HuggingFace
)
```

**ä¼˜ç‚¹ï¼š**
- âœ… è‡ªåŠ¨ä¸‹è½½å’Œç¼“å­˜
- âœ… æ”¯æŒæµ·é‡æ•°æ®é›†
- âœ… é«˜æ•ˆçš„æ•°æ®å¤„ç†

**ç¼ºç‚¹ï¼š**
- âŒ éœ€è¦å®‰è£…`datasets`åº“
- âŒ é¦–æ¬¡ä¸‹è½½éœ€è¦æ—¶é—´

#### æ–¹å¼2ï¼šä»æœ¬åœ°æ–‡ä»¶åŠ è½½

```python
dataset = load_dataset(
    dataset_name='wikitext',
    tokenizer=tokenizer,
    use_hf=False,
    local_path='./data/wiki.test.raw'  # æœ¬åœ°æ–‡ä»¶
)
```

**ä¼˜ç‚¹ï¼š**
- âœ… ä¸ä¾èµ–å¤–éƒ¨åº“
- âœ… å®Œå…¨ç¦»çº¿

**ç¼ºç‚¹ï¼š**
- âŒ éœ€è¦æ‰‹åŠ¨ä¸‹è½½æ•°æ®
- âŒ åŠŸèƒ½è¾ƒç®€å•

#### æ–¹å¼3ï¼šè™šæ‹Ÿæ•°æ®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰âœ¨æ¨èç”¨äºè°ƒè¯•

```python
dataset = load_dataset(
    dataset_name='dummy',
    tokenizer=tokenizer,
    max_samples=100
)
```

**ä¼˜ç‚¹ï¼š**
- âœ… æ— éœ€ä¸‹è½½
- âœ… æå¿«é€Ÿåº¦
- âœ… é€‚åˆè°ƒè¯•æ¡†æ¶

**ç¼ºç‚¹ï¼š**
- âŒ ä¸æ˜¯çœŸå®æ•°æ®

### ä¸full_example.pyçš„é›†æˆ

**æ›´æ–°å‰ï¼š**
```python
from datasets import load_dataset  # ä¾èµ–HuggingFace
```

**æ›´æ–°åï¼š**
```python
from custom_datasets import load_dataset  # ä½¿ç”¨ç»Ÿä¸€æ¥å£

# è‡ªåŠ¨fallbackï¼Œä¼˜å…ˆHFï¼Œå¤±è´¥åˆ™ç”¨è‡ªå®šä¹‰
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', ...)
```

### å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
from transformers import GPT2Tokenizer
from custom_datasets import prepare_wikitext2, prepare_dummy_data

# 1. å¿«é€Ÿæµ‹è¯•ï¼ˆæ¨èç”¨äºå¼€å‘ï¼‰
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
dataloader = prepare_dummy_data(
    tokenizer, 
    batch_size=2, 
    num_samples=10,
    seq_length=64
)

# 2. çœŸå®å®éªŒï¼ˆæ¨èç”¨äºæ­£å¼å®éªŒï¼‰
dataloader = prepare_wikitext2(
    tokenizer,
    batch_size=4,
    max_samples=100,
    seq_length=128,
    use_hf=True  # å°è¯•HFï¼Œå¤±è´¥åˆ™fallback
)

# 3. å®Œå…¨è‡ªå®šä¹‰
from custom_datasets import SimpleTextDataset, create_dataloader

texts = ["Your custom text here...", ...]
dataset = SimpleTextDataset(texts, tokenizer, max_length=128)
dataloader = create_dataloader(dataset, batch_size=4)
```

---

## é—®é¢˜2ï¼šç»Ÿä¸€å¼ é‡æ”¶é›†æœºåˆ¶ âœ…

### é—®é¢˜åˆ†æ

**ä¹‹å‰çš„è®¾è®¡é—®é¢˜ï¼š**

```
AttentionHook (åœ¨model_adapter.py)
    â”œâ”€â”€ capture() - æ•è·å¹¶cloneå¼ é‡
    â””â”€â”€ captured_tensors - å­˜å‚¨å‰¯æœ¬

IntermediateTensorCollector (åœ¨complete_experiment_runner.py)
    â”œâ”€â”€ add() - å†æ¬¡cloneå¼ é‡
    â””â”€â”€ tensors - å†æ¬¡å­˜å‚¨å‰¯æœ¬

é—®é¢˜ï¼šåŠŸèƒ½é‡å¤ï¼Œä¸¤æ¬¡cloneï¼Œå†…å­˜æµªè´¹ï¼
```

### æ–°çš„è®¾è®¡ âœ¨

#### æ˜ç¡®åˆ†å·¥

| ç»„ä»¶ | èŒè´£ | ä½ç½® |
|------|------|------|
| **AttentionHook** | â‘  æ‰§è¡Œæ³¨å…¥<br>â‘¡ æä¾›å¼ é‡å¼•ç”¨ | model_adapter.py |
| **IntermediateTensorCollector** | â‘  ä»å¼•ç”¨clone<br>â‘¡ ç»Ÿä¸€å­˜å‚¨ | complete_experiment_runner.py |

#### å·¥ä½œæµç¨‹

```
1. Forwardå¼€å§‹
   â†“
2. AttentionHook.register_tensor(name, tensor)
   - åªä¿å­˜å¼•ç”¨ï¼Œä¸clone
   â†“
3. AttentionHook.maybe_inject(name, tensor)
   - å¦‚æœé…ç½®äº†ï¼Œæ‰§è¡Œæ³¨å…¥
   â†“
4. è¿”å› (output, intermediates)
   - intermediatesåŒ…å«å¼ é‡å¼•ç”¨
   â†“
5. IntermediateTensorCollector.collect_from_intermediates(intermediates)
   - ä»å¼•ç”¨cloneå¹¶å­˜å‚¨
   â†“
6. å¤–éƒ¨è®¿é—®
   - collector.get_layer(layer_idx)
   - collector.get_all()
```

### ä»£ç å˜åŒ–

#### AttentionHookï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
class AttentionHook:
    """åªè´Ÿè´£æ³¨å…¥å’Œæä¾›å¼•ç”¨"""
    
    def __init__(self, injection_config=None):
        self.injection_config = injection_config
        self._temp_tensors = {}  # ä¸´æ—¶å¼•ç”¨ï¼Œä¸clone
    
    def register_tensor(self, name: str, tensor: torch.Tensor):
        """æ³¨å†Œå¼ é‡å¼•ç”¨ï¼ˆä¸å¤åˆ¶ï¼‰"""
        self._temp_tensors[name] = tensor  # åªä¿å­˜å¼•ç”¨
    
    def get_tensors(self) -> Dict[str, torch.Tensor]:
        """è¿”å›å¼•ç”¨å­—å…¸"""
        return self._temp_tensors
    
    def maybe_inject(self, name: str, tensor: torch.Tensor) -> bool:
        """æ‰§è¡Œæ³¨å…¥ï¼ˆå¦‚æœé…ç½®äº†ï¼‰"""
        if should_inject(name):
            bitflip_(tensor, ...)
            return True
        return False
```

#### IntermediateTensorCollectorï¼ˆè´Ÿè´£å­˜å‚¨ï¼‰

```python
class IntermediateTensorCollector:
    """è´Ÿè´£æ”¶é›†å’Œå­˜å‚¨å¼ é‡"""
    
    def __init__(self):
        self.tensors = {}  # {layer_idx: {name: tensor}}
        self.enabled = False
    
    def collect_from_intermediates(self, intermediates: Dict):
        """ä»adapterè¿”å›çš„å¼•ç”¨ä¸­æ”¶é›†"""
        layer_idx = intermediates['layer_idx']
        
        for name, tensor in intermediates.items():
            if isinstance(tensor, torch.Tensor):
                # åœ¨è¿™é‡Œcloneï¼Œåªcloneä¸€æ¬¡
                self.tensors[layer_idx][name] = tensor.detach().clone()
```

### å†…å­˜ä¼˜åŒ–

**ä¼˜åŒ–å‰ï¼š**
```python
# AttentionHookä¸­
captured = tensor.detach().clone()  # ç¬¬ä¸€æ¬¡clone

# IntermediateTensorCollectorä¸­
stored = captured.detach().clone()  # ç¬¬äºŒæ¬¡cloneï¼

å†…å­˜ä½¿ç”¨ï¼š2Ã— tensor size
```

**ä¼˜åŒ–åï¼š**
```python
# AttentionHookä¸­
reference = tensor  # åªä¿å­˜å¼•ç”¨

# IntermediateTensorCollectorä¸­
stored = reference.detach().clone()  # åªcloneä¸€æ¬¡

å†…å­˜ä½¿ç”¨ï¼š1Ã— tensor size
```

### ä½¿ç”¨æ–¹å¼

#### åœ¨complete_experiment_runner.pyä¸­

```python
# åˆ›å»ºcollector
collector = IntermediateTensorCollector()

# è¿è¡Œå¹¶æ”¶é›†
outputs, collected = run_model_with_collection(
    model, 
    input_ids, 
    attention_mask, 
    collector,
    injection_config=None  # baselineæˆ–æ³¨å…¥é…ç½®
)

# è®¿é—®æ”¶é›†çš„å¼ é‡
for layer_idx in collected.keys():
    layer_tensors = collected[layer_idx]
    q = layer_tensors['q']
    k = layer_tensors['k']
    scores = layer_tensors['scores']
    # ...è®¡ç®—è¾¹ç•Œ
```

### æ³¨æ„äº‹é¡¹

âš ï¸ **é‡è¦ï¼š** 

1. **AttentionHookä¸­çš„å¼•ç”¨åªåœ¨forwardæœŸé—´æœ‰æ•ˆ**
   - forwardä¹‹åï¼ŒåŸå§‹tensorå¯èƒ½è¢«ä¿®æ”¹
   - å¿…é¡»é€šè¿‡IntermediateTensorCollectoråŠæ—¶clone

2. **ä¸è¦ç›´æ¥è®¿é—®AttentionHookçš„_temp_tensors**
   - è¿™äº›æ˜¯ä¸´æ—¶å¼•ç”¨
   - ä½¿ç”¨IntermediateTensorCollectorçš„æ¥å£

3. **åœ¨å¤šå±‚æ³¨å…¥æ—¶**
   - æ¯å±‚æœ‰ç‹¬ç«‹çš„AttentionHook
   - IntermediateTensorCollectorç»Ÿä¸€æ”¶é›†æ‰€æœ‰å±‚

---

## æ›´æ–°çš„æ–‡ä»¶æ¸…å•

### æ–°å¢æ–‡ä»¶
- âœ¨ **custom_datasets.py** - æ•°æ®é›†åŠ è½½æ¨¡å—

### ä¿®æ”¹æ–‡ä»¶
- ğŸ”§ **model_adapter.py** - ç®€åŒ–AttentionHook
- ğŸ”§ **complete_experiment_runner.py** - æ›´æ–°IntermediateTensorCollector

---

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# æœ€å°ä¾èµ–ï¼ˆå¿…éœ€ï¼‰
pip install torch transformers

# å¯é€‰ä¾èµ–ï¼ˆæ¨èï¼‰
pip install datasets  # ç”¨äºHuggingFace datasets
pip install scipy numpy psutil  # ç”¨äºè¾¹ç•Œè®¡ç®—å’Œæ€§èƒ½ç›‘æ§
```

### 2. æµ‹è¯•æ•°æ®åŠ è½½

```python
# æµ‹è¯•custom_datasets.py
python custom_datasets.py

# åº”è¯¥çœ‹åˆ°3ä¸ªç¤ºä¾‹çš„è¾“å‡º
```

### 3. è¿è¡Œå®Œæ•´ç¤ºä¾‹

```python
# ä½¿ç”¨è™šæ‹Ÿæ•°æ®ï¼ˆæœ€å¿«ï¼Œæ— éœ€ä¸‹è½½ï¼‰
python full_example.py simple

# ä½¿ç”¨çœŸå®æ•°æ®ï¼ˆéœ€è¦datasetsåº“ï¼‰
# ä¼šè‡ªåŠ¨ä¸‹è½½WikiText-2
python full_example.py simple
```

---

## å¯¹æ¯”è¡¨

### æ•°æ®é›†åŠ è½½

| ç‰¹æ€§ | ä¹‹å‰ | ç°åœ¨ |
|------|------|------|
| HuggingFaceæ”¯æŒ | âœ… | âœ… |
| ç¦»çº¿æ”¯æŒ | âŒ | âœ… |
| å¿«é€Ÿæµ‹è¯• | âŒ | âœ… (dummy) |
| è‡ªå®šä¹‰æ•°æ® | âŒ | âœ… |
| ç»Ÿä¸€æ¥å£ | âŒ | âœ… |

### å¼ é‡æ”¶é›†

| ç‰¹æ€§ | ä¹‹å‰ | ç°åœ¨ |
|------|------|------|
| åŠŸèƒ½é‡å¤ | âŒ æ˜¯ | âœ… å¦ |
| Cloneæ¬¡æ•° | 2æ¬¡ | 1æ¬¡ |
| å†…å­˜ä½¿ç”¨ | 2Ã— | 1Ã— |
| ä»£ç æ¸…æ™°åº¦ | æ··ä¹± | æ¸…æ™° |
| ç»´æŠ¤æ€§ | å·® | å¥½ |

---

## å»ºè®®çš„ä½¿ç”¨é¡ºåº

### å¼€å‘é˜¶æ®µï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰

```python
# 1. ä½¿ç”¨è™šæ‹Ÿæ•°æ®
from custom_datasets import prepare_dummy_data

dataloader = prepare_dummy_data(
    tokenizer,
    batch_size=2,
    num_samples=10  # å¾ˆå°çš„æ•°æ®é‡
)

# å¿«é€Ÿæµ‹è¯•æ¡†æ¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
```

### è°ƒè¯•é˜¶æ®µï¼ˆå°è§„æ¨¡çœŸå®æ•°æ®ï¼‰

```python
# 2. ä½¿ç”¨WikiText-2ï¼ˆå°æ•°æ®é›†ï¼‰
from custom_datasets import prepare_wikitext2

dataloader = prepare_wikitext2(
    tokenizer,
    batch_size=4,
    max_samples=50,  # é™åˆ¶æ ·æœ¬æ•°
    use_hf=True
)

# éªŒè¯åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°
```

### å®éªŒé˜¶æ®µï¼ˆå®Œæ•´æ•°æ®ï¼‰

```python
# 3. ä½¿ç”¨å®Œæ•´æ•°æ®é›†
dataloader = prepare_wikitext2(
    tokenizer,
    batch_size=8,
    max_samples=None,  # ä½¿ç”¨æ‰€æœ‰æ•°æ®
    use_hf=True
)

# è¿è¡Œå®Œæ•´å®éªŒ
```

---

## å¸¸è§é—®é¢˜

### Q1: datasetsåº“å®‰è£…å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A:** ä½¿ç”¨å†…ç½®çš„dummyæˆ–è‡ªå®šä¹‰å®ç°ï¼š

```python
# ä¸ä¾èµ–datasetsåº“
dataloader = prepare_dummy_data(tokenizer, ...)
```

### Q2: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Ÿ

**A:** ä½¿ç”¨SimpleTextDatasetï¼š

```python
from custom_datasets import SimpleTextDataset, create_dataloader

my_texts = ["text 1", "text 2", ...]
dataset = SimpleTextDataset(my_texts, tokenizer, max_length=128)
dataloader = create_dataloader(dataset, batch_size=4)
```

### Q3: IntermediateTensorCollectorå ç”¨å¤ªå¤šå†…å­˜ï¼Ÿ

**A:** é™åˆ¶æ”¶é›†çš„å±‚æ•°ï¼š

```python
# åªåœ¨ç‰¹å®šå±‚æ³¨å…¥å’Œæ”¶é›†
config.injection_layers = [0, 1]  # åªæ”¶é›†å‰ä¸¤å±‚
```

æˆ–è€…ä½¿ç”¨æ›´å°çš„batch_sizeå’Œseq_lengthã€‚

### Q4: ä¸ºä»€ä¹ˆä¸ç›´æ¥ä½¿ç”¨AttentionHookå­˜å‚¨ï¼Ÿ

**A:** å› ä¸ºï¼š
1. AttentionHookåœ¨æ¯å±‚ç‹¬ç«‹åˆ›å»ºï¼Œéœ€è¦ç»Ÿä¸€ç®¡ç†
2. åˆ†ç¦»å…³æ³¨ç‚¹ï¼šæ³¨å…¥vsæ”¶é›†
3. é¿å…é‡å¤cloneï¼ŒèŠ‚çœå†…å­˜

---

## ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å¯ä»¥ï¼š

1. âœ… ä½¿ç”¨å¤šç§æ–¹å¼åŠ è½½æ•°æ®
2. âœ… ç†è§£å¼ é‡æ”¶é›†çš„æœºåˆ¶
3. âœ… è¿è¡Œå®Œæ•´çš„å®éªŒ

å»ºè®®ï¼š

```bash
# 1. å…ˆæµ‹è¯•æ•°æ®åŠ è½½
python custom_datasets.py

# 2. è¿è¡Œç®€å•å®éªŒ
python full_example.py simple

# 3. æ£€æŸ¥ç»“æœ
ls results/gpt2_simple_test_*/
```

ç¥å®éªŒé¡ºåˆ©ï¼ğŸ‰

# Bugä¿®å¤æŒ‡å—

## é—®é¢˜æè¿°

è¿è¡Œ`full_example.py`æ—¶é‡åˆ°é”™è¯¯ï¼š
```
AttributeError: 'GPT2Attention' object has no attribute '_split_heads'
```

## æ ¹æœ¬åŸå› 

åœ¨`model_adapter.py`çš„`GPT2AttentionAdapter`ä¸­ï¼Œä»£ç å°è¯•è°ƒç”¨`self.attn._split_heads()`æ–¹æ³•ï¼Œä½†è¿™ä¸ªæ–¹æ³•åœ¨ä¸åŒç‰ˆæœ¬çš„transformersåº“ä¸­å¯èƒ½ä¸å­˜åœ¨æˆ–å‘½åä¸åŒã€‚

## ä¿®å¤å†…å®¹

### 1. GPT2AttentionAdapterï¼ˆmodel_adapter.pyï¼‰âœ…

**ä¿®å¤å‰çš„é—®é¢˜ï¼š**
- ä¾èµ–GPT2Attentionçš„å†…éƒ¨æ–¹æ³•`_split_heads`å’Œ`_merge_heads`
- ä¸å…¼å®¹ä¸åŒç‰ˆæœ¬çš„transformers

**ä¿®å¤åï¼š**
- âœ… æ‰‹åŠ¨å®ç°`_split_heads`å’Œ`_merge_heads`æ–¹æ³•
- âœ… ä½¿ç”¨`getattr`å’Œ`hasattr`å®‰å…¨è®¿é—®å±æ€§
- âœ… æ·»åŠ è¯¦ç»†çš„æ­¥éª¤æ³¨é‡Š
- âœ… å®Œå…¨ç‹¬ç«‹ï¼Œä¸ä¾èµ–GPT2çš„å†…éƒ¨å®ç°

**å…³é”®ä»£ç ï¼š**
```python
def _split_heads(self, tensor, num_heads, head_dim):
    """æ‰‹åŠ¨å®ç°ï¼Œä¸ä¾èµ–åŸå§‹æ¨¡å‹"""
    batch_size, seq_length = tensor.size()[:2]
    tensor = tensor.view(batch_size, seq_length, num_heads, head_dim)
    return tensor.permute(0, 2, 1, 3)

def _merge_heads(self, tensor, num_heads, head_dim):
    """æ‰‹åŠ¨å®ç°"""
    tensor = tensor.permute(0, 2, 1, 3).contiguous()
    batch_size, seq_length = tensor.size()[:2]
    return tensor.view(batch_size, seq_length, num_heads * head_dim)
```

### 2. monkey_patch_modelï¼ˆmodel_adapter.pyï¼‰âœ…

**ä¿®å¤å‰çš„é—®é¢˜ï¼š**
- é—­åŒ…å˜é‡`layer_idx`å¯èƒ½æœ‰ä½œç”¨åŸŸé—®é¢˜
- æ²¡æœ‰æ­£ç¡®åˆå§‹åŒ–`_injection_config`

**ä¿®å¤åï¼š**
- âœ… ä½¿ç”¨`make_new_forward`å‡½æ•°æ­£ç¡®æ•è·`layer_idx`
- âœ… åˆå§‹åŒ–æ¯å±‚çš„`_injection_config`å±æ€§
- âœ… æ·»åŠ å®Œæˆæç¤º

**å…³é”®ä»£ç ï¼š**
```python
def make_new_forward(adp, idx):
    def new_forward(hidden_states, *args, **kwargs):
        inj_cfg = getattr(layer.attn, '_injection_config', None)
        output, intermediates = adp.forward_with_injection(
            hidden_states,
            layer_idx=idx,  # æ­£ç¡®æ•è·idx
            injection_config=inj_cfg,
            return_intermediates=True,
            *args, **kwargs
        )
        return output
    return new_forward

layer.attn.forward = make_new_forward(adapter, layer_idx)
```

### 3. run_model_with_collectionï¼ˆcomplete_experiment_runner.pyï¼‰âœ…

**ä¿®å¤å‰çš„é—®é¢˜ï¼š**
- wrapperåˆ›å»ºæœ‰é—®é¢˜
- æ²¡æœ‰æ¢å¤åŸå§‹forwardæ–¹æ³•

**ä¿®å¤åï¼š**
- âœ… ä½¿ç”¨try-finallyç¡®ä¿æ¢å¤
- âœ… æ­£ç¡®çš„wrapperå®ç°
- âœ… æ”¹è¿›çš„é”™è¯¯å¤„ç†

## æµ‹è¯•æ­¥éª¤

### æ­¥éª¤1ï¼šæœ€å°æµ‹è¯•

åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ–‡ä»¶`test_fix.py`ï¼š

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from model_adapter import monkey_patch_model, GPT2AttentionAdapter

print("Testing GPT2 Adapter Fix...")

# 1. åŠ è½½æ¨¡å‹
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

print("âœ“ Model loaded")

# 2. Patchæ¨¡å‹
model = monkey_patch_model(model, 'gpt2', injection_layers=[0])
print("âœ“ Model patched")

# 3. æµ‹è¯•forward
text = "Hello world"
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

print("âœ“ Forward pass successful")
print(f"Output shape: {outputs.logits.shape}")
print("\nğŸ‰ All tests passed!")
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
python test_fix.py
```

**æœŸæœ›è¾“å‡ºï¼š**
```
Testing GPT2 Adapter Fix...
âœ“ Model loaded
GPT2AttentionAdapter initialized: heads=12, head_dim=64
Patching 1 out of 12 attention layers
Layer indices: [0]
âœ“ Model patching completed
âœ“ Model patched
âœ“ Forward pass successful
Output shape: torch.Size([1, 2, 50257])

ğŸ‰ All tests passed!
```

### æ­¥éª¤2ï¼šå®Œæ•´æµ‹è¯•

è¿è¡Œå®Œæ•´çš„ç¤ºä¾‹ï¼š

```bash
python full_example.py simple
```

**æœŸæœ›çœ‹åˆ°ï¼š**
```
Loading gpt2...
Model loaded on cpu
...
GPT2AttentionAdapter initialized: heads=12, head_dim=64
Patching 1 out of 12 attention layers
Layer indices: [0]
âœ“ Model patching completed
...
[YYYY-MM-DD HH:MM:SS] [INFO] Starting run 0
...
âœ“ Experiment completed!
```

## éªŒè¯æ¸…å•

- [ ] `test_fix.py`è¿è¡ŒæˆåŠŸ
- [ ] `full_example.py simple`è¿è¡ŒæˆåŠŸ
- [ ] æ²¡æœ‰`AttributeError`
- [ ] èƒ½çœ‹åˆ°"GPT2AttentionAdapter initialized"æ¶ˆæ¯
- [ ] èƒ½çœ‹åˆ°"Model patching completed"æ¶ˆæ¯
- [ ] Forward passæˆåŠŸå®Œæˆ

## å…¼å®¹æ€§è¯´æ˜

ä¿®å¤åçš„ä»£ç å…¼å®¹ï¼š

| transformersç‰ˆæœ¬ | çŠ¶æ€ |
|------------------|------|
| 4.x (æœ€æ–°) | âœ… å®Œå…¨å…¼å®¹ |
| 3.x | âœ… åº”è¯¥å…¼å®¹ |
| 2.x | âš ï¸ æœªæµ‹è¯• |

## å¦‚æœä»ç„¶é‡åˆ°é—®é¢˜

### é—®é¢˜1ï¼šå…¶ä»–AttributeError

**æ£€æŸ¥ï¼š**
```python
# åœ¨test_fix.pyä¸­æ·»åŠ 
print(f"Has c_attn: {hasattr(model.transformer.h[0].attn, 'c_attn')}")
print(f"Has c_proj: {hasattr(model.transformer.h[0].attn, 'c_proj')}")
print(f"Attributes: {dir(model.transformer.h[0].attn)}")
```

### é—®é¢˜2ï¼šShapeä¸åŒ¹é…

**æ£€æŸ¥ï¼š**
```python
# åœ¨GPT2AttentionAdapter.__init__ä¸­æ·»åŠ 
print(f"num_heads: {self.num_heads}")
print(f"head_dim: {self.head_dim}")
print(f"split_size: {self.split_size}")
```

### é—®é¢˜3ï¼šæ³¨å…¥ä¸ç”Ÿæ•ˆ

**æ£€æŸ¥ï¼š**
```python
# åœ¨forward_with_injectionä¸­æ·»åŠ 
print(f"Injection config: {injection_config}")
print(f"Hook injection applied: {hook.injection_applied}")
```

## è°ƒè¯•æŠ€å·§

### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

åœ¨`model_adapter.py`çš„`forward_with_injection`å¼€å¤´æ·»åŠ ï¼š
```python
if layer_idx == 0:  # åªæ‰“å°ç¬¬0å±‚
    print(f"Layer {layer_idx}: input shape = {hidden_states.shape}")
```

### 2. æ£€æŸ¥ä¸­é—´å¼ é‡

åœ¨`forward_with_injection`ä¸­æ·»åŠ ï¼š
```python
if return_intermediates:
    print(f"Q shape: {query.shape}")
    print(f"K shape: {key.shape}")
    print(f"V shape: {value.shape}")
    print(f"Scores shape: {attn_weights.shape}")
```

### 3. éªŒè¯adapter

```python
# åœ¨monkey_patch_modelå
for i, layer in enumerate(model.transformer.h):
    if hasattr(layer.attn, 'adapter'):
        print(f"Layer {i}: has adapter = True")
        print(f"  adapter type: {type(layer.attn.adapter)}")
```

## æ€§èƒ½å½±å“

ä¿®å¤å¯¹æ€§èƒ½çš„å½±å“ï¼š
- âœ… å†…å­˜ï¼šæ— é¢å¤–å¼€é”€ï¼ˆä»ç„¶æ˜¯1Ã—cloneï¼‰
- âœ… é€Ÿåº¦ï¼šå¯èƒ½ç•¥æ…¢ï¼ˆæ‰‹åŠ¨reshape vsä¼˜åŒ–çš„å†…éƒ¨æ–¹æ³•ï¼‰
- âœ… ç²¾åº¦ï¼šå®Œå…¨ä¸€è‡´

## åç»­ä¼˜åŒ–

å¦‚æœéœ€è¦æ›´å¥½çš„æ€§èƒ½ï¼Œå¯ä»¥è€ƒè™‘ï¼š

1. **ç¼“å­˜reshapeæ“ä½œ**
2. **ä½¿ç”¨torch.jit.scriptç¼–è¯‘**
3. **æ‰¹é‡å¤„ç†å¤šä¸ªå±‚**

ä½†å¯¹äºå½“å‰çš„å®éªŒç›®çš„ï¼Œç°æœ‰å®ç°å·²ç»è¶³å¤Ÿå¥½äº†ã€‚

## æ€»ç»“

âœ… **é—®é¢˜å·²ä¿®å¤**
- ä¸å†ä¾èµ–transformerså†…éƒ¨æ–¹æ³•
- å…¼å®¹ä¸åŒç‰ˆæœ¬
- ä»£ç æ›´å¥å£®
- æ·»åŠ äº†å®‰å…¨æ£€æŸ¥

ğŸš€ **å¯ä»¥å¼€å§‹å®éªŒäº†ï¼**

è¿è¡Œï¼š
```bash
python full_example.py simple
```

åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼

# æœ€ç»ˆä¿®å¤æ€»ç»“

## é—®é¢˜åˆ—è¡¨

è¿è¡Œ`full_example.py`æ—¶é‡åˆ°çš„ä¸¤ä¸ªä¸»è¦é”™è¯¯ï¼š

### é”™è¯¯1: AttributeError '_split_heads' âœ… å·²ä¿®å¤
```
AttributeError: 'GPT2Attention' object has no attribute '_split_heads'
```

### é”™è¯¯2: AttributeError 'get' âœ… å·²ä¿®å¤  
```
AttributeError: 'Tensor' object has no attribute 'get'. Did you mean: 'det'?
```

## æ ¹æœ¬åŸå› åˆ†æ

### é—®é¢˜1çš„åŸå› 
- ä»£ç ä¾èµ–äº†`transformers`åº“çš„å†…éƒ¨ç§æœ‰æ–¹æ³•`_split_heads`å’Œ`_merge_heads`
- è¿™äº›æ–¹æ³•åœ¨ä¸åŒç‰ˆæœ¬ä¸­å¯èƒ½ä¸å­˜åœ¨æˆ–å‘½åä¸åŒ

### é—®é¢˜2çš„åŸå› ï¼ˆæ›´å¤æ‚ï¼‰

æœ‰**ä¸‰ä¸ª**å­é—®é¢˜ï¼š

#### 2.1 é—­åŒ…å˜é‡æ•è·é”™è¯¯
åœ¨`run_model_with_collection`çš„å¾ªç¯ä¸­ï¼š
```python
for layer in model.transformer.h:
    def make_forward_wrapper(...):
        def wrapper(...):
            # è¿™é‡Œå¼•ç”¨äº†å¤–éƒ¨çš„layerå˜é‡ï¼
            inj_cfg = getattr(layer.attn, '_injection_config', None)
            ...
```

**é—®é¢˜ï¼š** Pythonçš„é—­åŒ…æ•è·çš„æ˜¯å˜é‡å¼•ç”¨ï¼Œä¸æ˜¯å€¼ã€‚æ‰€æœ‰wrapperéƒ½ä¼šä½¿ç”¨æœ€åä¸€ä¸ª`layer`ï¼

#### 2.2 è¿”å›å€¼ä¸åŒ¹é…

GPT2çš„åŸå§‹APIï¼š
```python
attn_output, attn_weights = self.attn(hidden_states, ...)
```

æˆ‘ä»¬çš„wrapperåªè¿”å›ï¼š
```python
return output  # åªæœ‰ä¸€ä¸ªå€¼ï¼
```

**é—®é¢˜ï¼š** GPT2BlockæœŸæœ›ä¸¤ä¸ªè¿”å›å€¼ï¼Œä½†æˆ‘ä»¬åªè¿”å›ä¸€ä¸ªï¼Œå¯¼è‡´unpackingé”™è¯¯ã€‚

#### 2.3 _injection_configç»‘å®šå¤±è´¥

åœ¨`monkey_patch_model`çš„é—­åŒ…ä¸­ï¼š
```python
def new_forward(...):
    inj_cfg = getattr(layer.attn, '_injection_config', None)
```

**é—®é¢˜ï¼š** `layer`åœ¨é—­åŒ…åˆ›å»ºåå¯èƒ½å·²ç»æ”¹å˜ï¼Œå¯¼è‡´è·å–é”™è¯¯çš„å¯¹è±¡ã€‚

## ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤1: æ‰‹åŠ¨å®ç°split/merge heads âœ…

**æ–‡ä»¶ï¼š** `model_adapter.py`

```python
class GPT2AttentionAdapter:
    def _split_heads(self, tensor, num_heads, head_dim):
        """å®Œå…¨æ‰‹åŠ¨å®ç°ï¼Œä¸ä¾èµ–transformerså†…éƒ¨æ–¹æ³•"""
        batch_size, seq_length = tensor.size()[:2]
        tensor = tensor.view(batch_size, seq_length, num_heads, head_dim)
        return tensor.permute(0, 2, 1, 3)
    
    def _merge_heads(self, tensor, num_heads, head_dim):
        """å®Œå…¨æ‰‹åŠ¨å®ç°"""
        tensor = tensor.permute(0, 2, 1, 3).contiguous()
        batch_size, seq_length = tensor.size()[:2]
        return tensor.view(batch_size, seq_length, num_heads * head_dim)
```

**ä¼˜ç‚¹ï¼š**
- âœ… å®Œå…¨ç‹¬ç«‹ï¼Œä¸ä¾èµ–ä»»ä½•å†…éƒ¨æ–¹æ³•
- âœ… å…¼å®¹æ‰€æœ‰transformersç‰ˆæœ¬
- âœ… æ¸…æ™°æ˜“æ‡‚

### ä¿®å¤2.1: æ­£ç¡®çš„é—­åŒ…å˜é‡æ•è· âœ…

**æ–‡ä»¶ï¼š** `complete_experiment_runner.py`

**ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰ï¼š**
```python
for layer in model.transformer.h:
    def make_forward_wrapper(orig_fwd, l_idx, col):
        def wrapper(hidden_states, *args, **kwargs):
            # âŒ è¿™é‡Œçš„layeræ˜¯å¾ªç¯å˜é‡ï¼Œæ‰€æœ‰wrapperå…±äº«ï¼
            inj_cfg = getattr(layer.attn, '_injection_config', None)
            ...
            output, intermediates = layer.attn.adapter.forward_with_injection(...)
```

**ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰ï¼š**
```python
for layer in model.transformer.h:
    def make_forward_wrapper(attn_obj, adapter_obj, l_idx, col):
        def wrapper(hidden_states, *args, **kwargs):
            # âœ… é€šè¿‡å‚æ•°ä¼ é€’ï¼Œæ¯ä¸ªwrapperæœ‰ç‹¬ç«‹çš„å¯¹è±¡å¼•ç”¨
            inj_cfg = getattr(attn_obj, '_injection_config', None)
            ...
            output, intermediates = adapter_obj.forward_with_injection(...)
```

**å…³é”®æ”¹è¿›ï¼š**
- å°†`layer.attn`å’Œ`layer.attn.adapter`ä½œä¸ºå‚æ•°ä¼ é€’
- æ¯ä¸ªwrapperæ•è·ç‹¬ç«‹çš„å¯¹è±¡å¼•ç”¨
- é¿å…äº†é—­åŒ…é™·é˜±

### ä¿®å¤2.2: è¿”å›æ­£ç¡®çš„å€¼ âœ…

**æ–‡ä»¶ï¼š** `model_adapter.py` å’Œ `complete_experiment_runner.py`

**å…¼å®¹GPT2 APIï¼š**
```python
def wrapper(...):
    output, intermediates = adapter.forward_with_injection(...)
    
    # âœ… è¿”å›(output, weights)ä»¥å…¼å®¹GPT2
    if 'weights' in intermediates:
        return output, intermediates['weights']
    else:
        return output, None
```

**åœ¨ä¸¤ä¸ªåœ°æ–¹ä¿®å¤ï¼š**
1. `monkey_patch_model`ä¸­çš„`new_forward`
2. `run_model_with_collection`ä¸­çš„`wrapper`

### ä¿®å¤2.3: æ­£ç¡®è®¿é—®_injection_config âœ…

**æ–‡ä»¶ï¼š** `model_adapter.py`

**ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰ï¼š**
```python
def make_new_forward(adp, idx):
    def new_forward(...):
        # âŒ layeræ˜¯å¾ªç¯å˜é‡
        inj_cfg = getattr(layer.attn, '_injection_config', None)
```

**ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰ï¼š**
```python
def make_new_forward(adp, idx):
    def new_forward(...):
        # âœ… é€šè¿‡æ¨¡å‹ç»“æ„å’Œidxå®šä½åˆ°æ­£ç¡®çš„layer
        attn_obj = model.transformer.h[idx].attn
        inj_cfg = getattr(attn_obj, '_injection_config', None)
```

### é¢å¤–ä¿®å¤: Lossè®¡ç®— âœ…

**é—®é¢˜ï¼š** Baseline lossæ˜¯None

**åŸå› ï¼š** DummyDatasetæ²¡æœ‰æä¾›labels

**ä¿®å¤ï¼š** æ·»åŠ fallbackï¼Œä½¿ç”¨shifted input_idsä½œä¸ºtarget

```python
if labels is not None:
    # ä½¿ç”¨æä¾›çš„labels
    baseline_loss = F.cross_entropy(...)
else:
    # Fallback: ä½¿ç”¨language modelingçš„æ ‡å‡†åšæ³•
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = input_ids[..., 1:].contiguous()
    baseline_loss = F.cross_entropy(...)
```

## ä¿®æ”¹æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | é‡è¦æ€§ |
|------|----------|--------|
| **model_adapter.py** | â‘  æ‰‹åŠ¨å®ç°split/merge<br>â‘¡ ä¿®å¤è¿”å›å€¼<br>â‘¢ ä¿®å¤é—­åŒ… | â­â­â­ å…³é”® |
| **complete_experiment_runner.py** | â‘  ä¿®å¤é—­åŒ…<br>â‘¡ ä¿®å¤è¿”å›å€¼<br>â‘¢ æ”¹è¿›lossè®¡ç®— | â­â­â­ å…³é”® |
| **quick_test.py** | æ–°å¢æµ‹è¯•è„šæœ¬ | â­â­ é‡è¦ |
| **FINAL_FIX_SUMMARY.md** | æœ¬æ–‡æ¡£ | â­ æœ‰ç”¨ |

## æµ‹è¯•æ­¥éª¤

### æ­¥éª¤1: å¿«é€Ÿæµ‹è¯•ï¼ˆæ¨èï¼‰

```bash
python quick_test.py
```

**æœŸæœ›è¾“å‡ºï¼š**
```
[Test 1] Loading and patching model...
âœ“ Model loaded
âœ“ Model patched successfully

[Test 2] Testing forward pass...
âœ“ Forward pass successful

[Test 3] Testing return values...
âœ“ Attention returns tuple with 2 elements

[Test 4] Testing data loading...
âœ“ DataLoader works

[Test 5] Testing IntermediateTensorCollector...
âœ“ Collection successful

[Test 6] Testing error injection...
âœ“ Injection successful

ğŸ‰ All tests passed!
```

### æ­¥éª¤2: å®Œæ•´å®éªŒ

```bash
python full_example.py simple
```

**æœŸæœ›çœ‹åˆ°ï¼š**
- âœ“ æ²¡æœ‰AttributeError
- âœ“ Baseline lossæœ‰å€¼ï¼ˆä¸æ˜¯Noneï¼‰
- âœ“ Injected lossæœ‰å€¼
- âœ“ å®éªŒæˆåŠŸå®Œæˆ

## æŠ€æœ¯ç»†èŠ‚

### Pythoné—­åŒ…é™·é˜±

**é”™è¯¯ç¤ºä¾‹ï¼š**
```python
funcs = []
for i in range(3):
    def f():
        print(i)  # âŒ æ‰€æœ‰å‡½æ•°éƒ½æ‰“å°2ï¼ˆæœ€åçš„iå€¼ï¼‰
    funcs.append(f)

for f in funcs:
    f()  # è¾“å‡º: 2, 2, 2
```

**æ­£ç¡®åšæ³•ï¼š**
```python
funcs = []
for i in range(3):
    def make_f(x):
        def f():
            print(x)  # âœ… æ¯ä¸ªå‡½æ•°æ•è·ç‹¬ç«‹çš„xå€¼
        return f
    funcs.append(make_f(i))

for f in funcs:
    f()  # è¾“å‡º: 0, 1, 2
```

### GPT2 Attention API

åŸå§‹GPT2çš„attentionå¯èƒ½è¿”å›ï¼š
```python
# æƒ…å†µ1: åªè¿”å›output
attn_output = self.attn(hidden_states)

# æƒ…å†µ2: è¿”å›(output, weights)
attn_output, attn_weights = self.attn(hidden_states)
```

æˆ‘ä»¬çš„å®ç°ç»Ÿä¸€è¿”å›`(output, weights)`ï¼Œå…¼å®¹ä¸¤ç§æƒ…å†µã€‚

## éªŒè¯æ¸…å•

è¿è¡Œæµ‹è¯•åï¼Œæ£€æŸ¥ï¼š

- [ ] âœ… `quick_test.py`å…¨éƒ¨é€šè¿‡
- [ ] âœ… æ²¡æœ‰`AttributeError: '_split_heads'`
- [ ] âœ… æ²¡æœ‰`AttributeError: 'Tensor' object has no attribute 'get'`
- [ ] âœ… Baseline lossæœ‰å€¼ï¼ˆä¸æ˜¯Noneï¼‰
- [ ] âœ… Injected lossæœ‰å€¼
- [ ] âœ… Loss diffè¢«æ­£ç¡®è®¡ç®—
- [ ] âœ… èƒ½æ”¶é›†åˆ°ä¸­é—´å¼ é‡
- [ ] âœ… èƒ½æ£€æµ‹åˆ°æ³¨å…¥

## æ€§èƒ½å½±å“

| æ–¹é¢ | å½±å“ | è¯´æ˜ |
|------|------|------|
| **å†…å­˜** | æ— å˜åŒ– | ä»ç„¶æ˜¯1Ã—clone |
| **é€Ÿåº¦** | ç•¥æ…¢ | æ‰‹åŠ¨reshape vsä¼˜åŒ–çš„å†…éƒ¨æ–¹æ³•ï¼ˆå¯å¿½ç•¥ï¼‰|
| **ç²¾åº¦** | å®Œå…¨ä¸€è‡´ | æ•°å­¦ä¸Šç­‰ä»· |
| **å…¼å®¹æ€§** | â¬†ï¸ å¤§å¹…æå‡ | æ”¯æŒæ‰€æœ‰transformersç‰ˆæœ¬ |
| **ç¨³å®šæ€§** | â¬†ï¸ å¤§å¹…æå‡ | é¿å…äº†é—­åŒ…é™·é˜± |

## åç»­ä¼˜åŒ–å»ºè®®

### çŸ­æœŸï¼ˆ1-2å¤©ï¼‰
- [ ] æ·»åŠ æ›´å¤šå•å…ƒæµ‹è¯•
- [ ] æµ‹è¯•å…¶ä»–æ¨¡å‹ï¼ˆDistilBERT, OPTï¼‰
- [ ] ä¼˜åŒ–lossè®¡ç®—çš„fallbacké€»è¾‘

### ä¸­æœŸï¼ˆ1å‘¨ï¼‰
- [ ] æ·»åŠ æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] ä¼˜åŒ–å¤§è§„æ¨¡å®éªŒçš„å†…å­˜ä½¿ç”¨
- [ ] æ”¹è¿›é”™è¯¯æ¶ˆæ¯å’Œè°ƒè¯•ä¿¡æ¯

### é•¿æœŸï¼ˆ1æœˆï¼‰
- [ ] æ”¯æŒæ›´å¤šæ¨¡å‹æ¶æ„
- [ ] å®ç°è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶
- [ ] ä¼˜åŒ–è¾¹ç•Œè®¡ç®—æ€§èƒ½

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆbaseline lossæ˜¯Noneï¼Ÿ
**A:** DummyDatasetæ²¡æœ‰æä¾›labelsã€‚å·²æ·»åŠ fallbackä½¿ç”¨shifted input_idsã€‚

### Q: ä¸ºä»€ä¹ˆæ³¨å…¥ålosså˜åŒ–å¾ˆå°ï¼Ÿ
**A:** 
- å•æ¯”ç‰¹ç¿»è½¬å½±å“å¯èƒ½å¾ˆå°
- éœ€è¦æ‰«æå¤šä¸ªæ¯”ç‰¹ä½å’Œä½ç½®
- æŸäº›ä½ç½®çš„æ³¨å…¥å½±å“è¾ƒå¤§

### Q: å¦‚ä½•è°ƒè¯•é—­åŒ…é—®é¢˜ï¼Ÿ
**A:** åœ¨å‡½æ•°å†…æ·»åŠ printï¼š
```python
def wrapper(...):
    print(f"Layer idx in wrapper: {l_idx}")
    print(f"Adapter object: {adapter_obj}")
```

### Q: å¦‚ä½•éªŒè¯æ³¨å…¥æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ
**A:** æ£€æŸ¥collectedä¸­çš„`injection_applied`å­—æ®µï¼š
```python
if 'injection_applied' in collected[layer_idx]:
    print(f"Injection applied: {collected[layer_idx]['injection_applied']}")
```

## æ€»ç»“

âœ… **æ‰€æœ‰é—®é¢˜å·²ä¿®å¤**

**å…³é”®æ”¹è¿›ï¼š**
1. âœ… å®Œå…¨ç‹¬ç«‹çš„å®ç°ï¼ˆä¸ä¾èµ–transformerså†…éƒ¨ï¼‰
2. âœ… æ­£ç¡®çš„é—­åŒ…å¤„ç†ï¼ˆé¿å…å˜é‡å…±äº«ï¼‰
3. âœ… å…¼å®¹çš„APIï¼ˆè¿”å›æ­£ç¡®çš„å€¼ï¼‰
4. âœ… æ”¹è¿›çš„lossè®¡ç®—ï¼ˆfallbackæœºåˆ¶ï¼‰

ğŸš€ **ç°åœ¨å¯ä»¥å¼€å§‹å®éªŒäº†ï¼**

**å»ºè®®çš„æµ‹è¯•é¡ºåºï¼š**
```bash
# 1. å¿«é€Ÿæµ‹è¯•ï¼ˆ30ç§’ï¼‰
python quick_test.py

# 2. ç®€å•å®éªŒï¼ˆ2-3åˆ†é’Ÿï¼‰
python full_example.py simple

# 3. å‚æ•°æ‰«æï¼ˆæ›´é•¿æ—¶é—´ï¼‰
python full_example.py sweep
```

**å¦‚æœé‡åˆ°é—®é¢˜ï¼š**
1. æŸ¥çœ‹`quick_test.py`çš„è¾“å‡º
2. æ£€æŸ¥é”™è¯¯æ ˆçš„å…·ä½“è¡Œå·
3. æ·»åŠ printè°ƒè¯•
4. å‚è€ƒæœ¬æ–‡æ¡£çš„"å¸¸è§é—®é¢˜"éƒ¨åˆ†

ç¥å®éªŒé¡ºåˆ©ï¼ğŸ‰